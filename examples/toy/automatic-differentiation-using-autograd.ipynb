{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using autograd to calculate the gradient of a log-likelihood\n",
    "\n",
    "It is straightforward to use the automatic differentiation library [autograd](https://github.com/HIPS/autograd) to take the derivative of log-likelihoods defined in pints. Below is an example of how to do this.\n",
    "\n",
    "WARNING: We currently find this method of caculating model sensitivities to be quite slow for most time-series models, and so do not recommended it for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pints\n",
    "import pints.toy as toy\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    import autograd.numpy as np\n",
    "    from autograd.scipy.integrate import odeint\n",
    "    from autograd.builtins import tuple\n",
    "    from autograd import grad\n",
    "except ImportError:\n",
    "    print(\"\"\"This example requires autograd, which is not a pints dependency.\n",
    "    If you see this warning, try `pip install autograd`\"\"\")\n",
    "    exit(0)\n",
    "\n",
    "\n",
    "from timeit import repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin be defining a model, identical to the [Fitzhugh Nagumo](https://pints.readthedocs.io/en/latest/toy/fitzhugh_nagumo_model.html) toy model implemented in pints. The corresponding toy model in pints has its `evaluateS1()` method defined, so we can compare the results using automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoGradFitzhughNagumoModel(pints.ForwardModel):\n",
    "    def simulate(self, parameters, times):\n",
    "        y0 = np.array([-1, 1], dtype=float)\n",
    "        \n",
    "        def rhs(y, t, p):\n",
    "            V, R = y\n",
    "            a, b, c = p\n",
    "            dV_dt = (V - V**3 / 3 + R) * c\n",
    "            dR_dt = (V - a + b * R) / -c\n",
    "            return np.array([dV_dt, dR_dt])\n",
    "        \n",
    "        return odeint(rhs, y0, times, tuple((parameters,)))\n",
    "    \n",
    "    def n_parameters(self):\n",
    "        return 3\n",
    "    \n",
    "    def n_outputs(self):\n",
    "        return 2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap an existing pints likelihood class, and use the `autograd.grad` function to calculate the gradient of the given log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoGradLogLikelihood(pints.ProblemLogLikelihood):\n",
    "    def __init__(self, likelihood):\n",
    "        self.likelihood = likelihood\n",
    "        f = lambda x: self.likelihood(x)\n",
    "        self.likelihood_grad = grad(f)\n",
    "    def __call__(self, x):\n",
    "        return self.likelihood(x)\n",
    "    def evaluateS1(self, x):\n",
    "        values = self.likelihood(x)\n",
    "        gradient = self.likelihood_grad(x)\n",
    "        return values, gradient\n",
    "    def n_parameters(self):\n",
    "        return self.likelihood.n_parameters()\n",
    "    \n",
    "autograd_model = AutoGradFitzhughNagumoModel()\n",
    "pints_model = pints.toy.FitzhughNagumoModel()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create some toy data and ensure that the new model gives the same output as the toy model in pints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some toy data\n",
    "real_parameters = np.array(pints_model.suggested_parameters(), dtype='float64')\n",
    "times = pints_model.suggested_times()\n",
    "pints_values = pints_model.simulate(real_parameters, times)\n",
    "autograd_values = autograd_model.simulate(real_parameters, times)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(times, autograd_values)\n",
    "plt.plot(times, pints_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some noise to the values, and then create log-likelihoods using both the new model, and the pints model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.1\n",
    "values = pints_values + np.random.normal(0, noise, pints_values.shape)\n",
    "\n",
    "# Create an object with links to the model and time series\n",
    "autograd_problem = pints.MultiOutputProblem(autograd_model, times, values)\n",
    "pints_problem = pints.MultiOutputProblem(pints_model, times, values)\n",
    "\n",
    "# Create a log-likelihood function\n",
    "autograd_log_likelihood = pints.GaussianKnownSigmaLogLikelihood(autograd_problem, noise)\n",
    "autograd_likelihood = AutoGradLogLikelihood(autograd_log_likelihood)\n",
    "pints_log_likelihood = pints.GaussianKnownSigmaLogLikelihood(pints_problem, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the gradients of both likelihood functions at the given parameters to make sure that they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograd_likelihood.evaluateS1(real_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pints_log_likelihood.evaluateS1(real_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll time both functions. You can see that the function using `autgrad` is significantly slower than the in-built `evaluateS1` function for the pints model. For reference, this function uses forward-mode sensitivity calculation using the symbolic Jacobian of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = 'autograd_likelihood.evaluateS1(real_parameters)'\n",
    "setup = 'from __main__ import autograd_likelihood, real_parameters'\n",
    "\n",
    "time_taken = min(repeat(stmt=statement, setup=setup, number=1, repeat=5))\n",
    "\n",
    "'Elapsed time: {:.0f} ms'.format(1000. * time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = 'pints_log_likelihood.evaluateS1(real_parameters)'\n",
    "setup = 'from __main__ import pints_log_likelihood, real_parameters'\n",
    "\n",
    "time_taken = min(repeat(stmt=statement, setup=setup, number=1, repeat=5))\n",
    "\n",
    "'Elapsed time: {:.0f} ms'.format(1000. * time_taken)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

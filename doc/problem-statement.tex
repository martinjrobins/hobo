%%
%% MCMC methods
%%
\documentclass[preprint,authoryear,12pt]{elsarticle}

\journal{the Journal of Nothing At All}

% We're in Europe
\usepackage[a4paper]{geometry}

% Specify pdf details
\pdfminorversion=7

% Print or electronic?
\usepackage{etoolbox}
\newtoggle{electronic}
\toggletrue{electronic}

% Make cross-refs and citations clickable
\usepackage{hyperref}
\iftoggle{electronic}
{
    % Electronic: use colors
    \hypersetup{
        colorlinks,
        linkcolor=blue,
        citecolor=blue
        }
}{
    % Print: don't indicate links at all
    \hypersetup{
        colorlinks=false,
        pdfborder=0 0 0
        }
}

% Note: Typing "hi. mike" makes tex use a long space for separating sentences
% This can be avoided using "hi.\ mike". Using "fig.~1" also works and has the
% extra advantage of preventing line breaks.
% Figure reference command. Usage: \fig{figure-one} or \fig[B]{figure-one}
\usepackage{etoolbox} % Needed for ifstrempty
\newcommand\fig[2][]{\hyperref[fig:#2]{Fig.~\ref*{fig:#2}\ifstrempty{#1}{}{.#1}}}
\newcommand\Fig[2][]{\hyperref[fig:#2]{Fig.~\ref*{fig:#2}\ifstrempty{#1}{}{.#1}}}

% Equation reference command
\newcommand{\eq}[1]{\hyperref[eq:#1]{equation \ref*{eq:#1}}}
\newcommand{\Eq}[1]{\hyperref[eq:#1]{Equation \ref*{eq:#1}}}

% Nice maths and algorithms
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% Numbers
\newcommand\real{{\rm I\!R}}

% Distributions
\newcommand\normal{\mathcal{N}}
\newcommand\uniform{\mathcal{U}}

% Figure placement for preprint
\usepackage{float}

% Vertical spacing instead of paragraph indents
\usepackage{parskip}

% Push long words onto the next line
\newcommand\lword[1]{
    \leavevmode\nobreak\hskip0pt plus\linewidth\penalty50\hskip
    0pt plus-\linewidth\nobreak{#1}}

% Inline code snippets
\newcommand{\code}[1]{\texttt{#1}}

% Table header
\newcommand*{\thead}[1]{\bfseries #1}

\begin{document}

\begin{frontmatter}

\title{Probabilistic Inference on Noisy Time Series}

%% Group authors per affiliation
%% or include affiliations in footnotes:
\author[csaddress]{PINTS}
\ead{pints@cs.ox.ac.uk}

\address[csaddress]{Department of Computer Science, University of Oxford,
Wolfson Building, Parks Road, Oxford, OX1 3QD, UK}


%
%
% Abstract
%
%
\begin{abstract}
Stuff
\end{abstract}

\end{frontmatter}


%
%
% Introduction
%
%
\section{Introduction}

We're trying to gather a bunch of algorithms to infer model parameters based
 on noisy time-series data.

%
% Problem statement
%
\subsection{Problem statement}

We have some noisy time-series data and a forward model (simulation) that can
 be used to replicate it.
We'd like to find out which parameter values are compatible with the
 experimental evidence.

\begin{itemize}
\item Observations $D = \{(t_1, z_1),...,(t_n, z_n)\}$ where $t$ is time
 ($t_i > t_{i-1}$ for $i > 1$) and $z_i\in\real$ is a noisy measurement at
 time $t_i$.
\item Forward model $f(t|\theta) \to \real$ with $m$ parameters, bundled in
 $\theta$.
\item The parameters live in some bounded space
 $\theta\in\Theta\subset\real^m$
\item \emph{Only} normally distributed, time-independent noise with some
 unknown variance $\sigma^2$, such that
 $z_i = f(t_i|\theta_{true}) + \normal(0, \sigma^2)$.
\end{itemize}

% Log-likelihood
\subsubsection{Log likelihood}

Some PINTS methods use the log-likelihood of the observations, given a proposed
 set of parameters $\theta$.
We define the likelihood in terms of the probability \emph{density} of
 obtaining observations $D$ for parameters $\theta$:
\begin{equation}
l(\theta|D) \equiv f(D|\theta)
\end{equation}
With purely random noise, the measurement error at any point in time is
 independent of the signal history, so that:
\begin{equation}
f(D|\theta) = \prod_{i=1}^{n} f(t_i,z_i|\theta)
\end{equation}
With normally distributed noise, the probability density function for
 observing $D$ is then:
\begin{equation}
f(x|\theta,\sigma) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}}
    \exp \left(
        -\frac{\left(x_i - f(t_i|\theta)\right)^2}{2\sigma^2}
    \right)
\end{equation}
And taking the logarithm, we find
\begin{equation}
\log l(\theta,\sigma|D) =
    - \frac{n}{2} \log(2\pi)
    - n \log(\sigma)
    - \frac{1}{2\sigma^2} \sum_{i=1}^n \left(z_i - f(t_i|\theta) \right)^2
\end{equation}
so that the likelihood is maximised for
\begin{equation}
\min_{ \theta, \sigma^2 }
    \left[
        2n\sigma^2 \log(\sigma)
        + \sum_{i=1}^n \left(z_i - f(t_i|\theta) \right)^2
    \right]
\end{equation}

We typically sneak $\sigma$ into the parameter vector (and increment the
 number of parameters $m$ by 1) to simply write $\log l(\theta|D)$.

% Cyclic voltammetry
\subsubsection{Cyclic voltammetry}

It's fun.
 

% Ionic currents
\subsubsection{Ionic currents}

Models of ionic currents are based on ODEs, so that the forward model involves
 solving an initial-value problem
 $y(t|\theta) = \int_{t_0}^t \dot{y}(\tau|\theta)d\tau$.
A few extra problems occur:
\begin{enumerate}
\item The ODE is forced/driven/paced by a function $q(t)$
\item The state of the ODE is not observed directly, but via a function
 $r(t, q(t), y(t|\theta), \theta)$.
\item The initial state $(t_0, y(t_0))$ is only approximately known.
\end{enumerate}
The third problem is usually ignored, leaving
\begin{equation}
f(t|\theta) = r(t, q(t), \int\dot{y}(\tau, \theta)d\tau, \theta)
\end{equation}

Finally, the noise is probably not normally distributed or independent, and the
 models often don't fit.






%
%
% References
%
%
\section*{References}

\bibliographystyle{model2-names}
\bibliography{references}

\end{document}
